# 三阶段筛选说明

## 功能概述

论文筛选采用**三阶段筛选机制**，通过逐步精细化筛选，最大化节省大模型token使用：

1. **阶段1: 关键词粗筛** - 使用关键词匹配快速过滤明显不相关的论文
2. **阶段2: 标题LLM筛选** - 仅使用论文标题进行LLM筛选（token消耗少）
3. **阶段3: 标题+摘要LLM筛选** - 对通过阶段2的论文使用标题+摘要进行精确筛选（token消耗大）

## 工作原理

### 阶段1: 关键词粗筛

- **目的**：快速过滤明显不相关的论文
- **方法**：关键词匹配（大小写不敏感）
- **阈值**：`coarse_filter_threshold`（默认：0.3）
- **Token消耗**：0（无需LLM调用）

### 阶段2: 标题LLM筛选

- **目的**：基于标题进行初步LLM判断，过滤标题明显不相关的论文
- **方法**：仅使用论文标题调用LLM
- **阈值**：`title_filter_threshold`（默认：0.5）
- **Token消耗**：低（仅标题，约100-200 tokens/篇）

### 阶段3: 标题+摘要LLM筛选

- **目的**：对通过阶段2的论文进行精确判断
- **方法**：使用标题+摘要调用LLM
- **阈值**：`relevance_threshold`（默认：0.7）
- **Token消耗**：高（标题+摘要，约500-1000 tokens/篇）

## 筛选流程

```
原始论文 (100篇)
    ↓
[阶段1] 关键词匹配 (threshold=0.3)
    ├─ 通过 (30篇) → [阶段2] 标题LLM筛选 (threshold=0.5)
    │                      ├─ 通过 (15篇) → [阶段3] 标题+摘要LLM筛选 (threshold=0.7) → 最终相关 (10篇)
    │                      └─ 未通过 (15篇) → 标记为不相关
    └─ 未通过 (70篇) → 直接标记为不相关
```

## 配置说明

在 `config.json` 的 `filter` 配置项中：

```json
{
  "filter": {
    "provider": "deepseek",
    "api_key": "YOUR_API_KEY",
    "relevance_threshold": 0.7,        // 阶段3阈值（标题+摘要）
    "title_filter_threshold": 0.5,     // 阶段2阈值（仅标题）
    "coarse_filter_threshold": 0.3,    // 阶段1阈值（关键词匹配）
    "enable_coarse_filter": true,      // 是否启用阶段1
    "keywords": ["HPC", "distributed computing"]
  }
}
```

### 参数说明

- **`relevance_threshold`** (0-1): 阶段3阈值
  - 用于标题+摘要LLM筛选
  - 越高越严格，默认0.7
  - 建议范围：0.6-0.8

- **`title_filter_threshold`** (0-1): 阶段2阈值
  - 用于仅标题LLM筛选
  - 中等严格度，默认0.5
  - 建议范围：0.4-0.6
  - 设置过低可能导致过多论文进入阶段3，设置过高可能漏掉相关论文

- **`coarse_filter_threshold`** (0-1): 阶段1阈值
  - 用于关键词匹配预筛选
  - 越低越宽松，默认0.01
  - 建议范围：0.2-0.4

- **`enable_coarse_filter`** (bool): 是否启用阶段1
  - `true`: 启用三阶段筛选（推荐）
  - `false`: 跳过阶段1，直接进入阶段2

## Token节省效果

假设：
- 阶段2（仅标题）：150 tokens/篇
- 阶段3（标题+摘要）：800 tokens/篇

### 场景1: 三阶段筛选（推荐）

| 阶段 | 论文数 | LLM调用 | Token消耗 | 累计 |
|------|--------|---------|-----------|------|
| 阶段1 | 100 | 0 | 0 | 0 |
| 阶段2 | 30 | 30 | 4,500 | 4,500 |
| 阶段3 | 15 | 15 | 12,000 | 16,500 |
| **总计** | **100** | **45** | **16,500** | - |

### 场景2: 两阶段筛选（旧方式）

| 阶段 | 论文数 | LLM调用 | Token消耗 | 累计 |
|------|--------|---------|-----------|------|
| 阶段1 | 100 | 0 | 0 | 0 |
| 阶段2 | 30 | 30 | 24,000 | 24,000 |
| **总计** | **100** | **30** | **24,000** | - |

**节省效果**：相比两阶段筛选，节省约 **31%** 的token（16,500 vs 24,000）

### 场景3: 单阶段筛选（无粗筛）

| 阶段 | 论文数 | LLM调用 | Token消耗 |
|------|--------|---------|-----------|
| LLM筛选 | 100 | 100 | 80,000 |

**节省效果**：相比单阶段筛选，节省约 **79%** 的token（16,500 vs 80,000）

## 日志输出

筛选过程会输出详细日志：

```
阶段1: 粗筛（关键词匹配，阈值: 0.30）...
阶段1完成: 30/100 篇论文通过粗筛
阶段2: 标题LLM筛选（阈值: 0.50）...
阶段2完成: 15/30 篇论文通过标题筛选
阶段3: 标题+摘要LLM筛选（阈值: 0.70）...
筛选完成: 10/100 篇论文最终相关
Token节省统计:
  - 阶段1过滤: 70 篇论文（关键词匹配，无需LLM）
  - 阶段2过滤: 15 篇论文（仅标题LLM，节省摘要token）
  - 总LLM调用: 45 次（阶段2: 30次，阶段3: 15次）
  - 相比单阶段筛选，节省约 55.0% 的完整LLM调用
```

## 使用示例

### 示例1: 标准三阶段筛选

```json
{
  "filter": {
    "provider": "deepseek",
    "api_key": "YOUR_KEY",
    "relevance_threshold": 0.7,
    "title_filter_threshold": 0.5,
    "coarse_filter_threshold": 0.3,
    "enable_coarse_filter": true,
    "keywords": ["HPC", "distributed computing"]
  }
}
```

### 示例2: 禁用阶段1（两阶段筛选）

```json
{
  "filter": {
    "enable_coarse_filter": false,
    "title_filter_threshold": 0.5,
    "relevance_threshold": 0.7
  }
}
```

### 示例3: 调整阈值

```json
{
  "filter": {
    "coarse_filter_threshold": 0.2,   // 阶段1更宽松
    "title_filter_threshold": 0.6,     // 阶段2更严格
    "relevance_threshold": 0.8         // 阶段3最严格
  }
}
```

## 注意事项

1. **阈值设置建议**：
   - 阶段1（关键词）：0.2-0.4，用于快速过滤
   - 阶段2（标题）：0.4-0.6，中等严格度
   - 阶段3（标题+摘要）：0.6-0.8，最严格

2. **性能平衡**：
   - 阶段2阈值低 → 更多论文进入阶段3 → 更准确但token消耗大
   - 阶段2阈值高 → 更少论文进入阶段3 → 节省token但可能漏掉相关论文

3. **向后兼容**：
   - 如果未配置 `title_filter_threshold`，默认使用0.5
   - 旧配置会自动使用默认值

4. **错误处理**：
   - 如果LLM调用失败，自动回退到关键词匹配
   - 各阶段独立，一个阶段失败不影响其他阶段

## 最佳实践

1. **首次使用**：使用默认阈值（0.3, 0.5, 0.7）
2. **观察效果**：查看日志中的token节省统计
3. **调整优化**：
   - 如果阶段2通过率>70%，考虑提高 `title_filter_threshold`
   - 如果阶段2通过率<30%，考虑降低 `title_filter_threshold`
   - 如果最终相关论文太少，考虑降低各阶段阈值

## 优势总结

1. **最大化Token节省**：通过三阶段逐步筛选，大幅减少完整LLM调用
2. **提高筛选效率**：标题筛选快速过滤，只对潜在相关论文进行详细分析
3. **灵活配置**：可根据需求调整各阶段阈值
4. **智能回退**：LLM失败时自动使用关键词匹配

